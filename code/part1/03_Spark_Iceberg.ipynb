{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fb5f2be-509c-41f0-813f-0948a9772751",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#****************************************************************************\n",
    "# (C) Cloudera, Inc. 2020-2023\n",
    "#  All rights reserved.\n",
    "#\n",
    "#  Applicable Open Source License: GNU Affero General Public License v3.0\n",
    "#\n",
    "#  NOTE: Cloudera open source products are modular software products\n",
    "#  made up of hundreds of individual components, each of which was\n",
    "#  individually copyrighted.  Each Cloudera open source product is a\n",
    "#  collective work under U.S. Copyright Law. Your license to use the\n",
    "#  collective work is as provided in your written agreement with\n",
    "#  Cloudera.  Used apart from the collective work, this file is\n",
    "#  licensed for your use pursuant to the open source license\n",
    "#  identified above.\n",
    "#\n",
    "#  This code is provided to you pursuant a written agreement with\n",
    "#  (i) Cloudera, Inc. or (ii) a third-party authorized to distribute\n",
    "#  this code. If you do not have a written agreement with Cloudera nor\n",
    "#  with an authorized and properly licensed third party, you do not\n",
    "#  have any rights to access nor to use this code.\n",
    "#\n",
    "#  Absent a written agreement with Cloudera, Inc. (“Cloudera”) to the\n",
    "#  contrary, A) CLOUDERA PROVIDES THIS CODE TO YOU WITHOUT WARRANTIES OF ANY\n",
    "#  KIND; (B) CLOUDERA DISCLAIMS ANY AND ALL EXPRESS AND IMPLIED\n",
    "#  WARRANTIES WITH RESPECT TO THIS CODE, INCLUDING BUT NOT LIMITED TO\n",
    "#  IMPLIED WARRANTIES OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY AND\n",
    "#  FITNESS FOR A PARTICULAR PURPOSE; (C) CLOUDERA IS NOT LIABLE TO YOU,\n",
    "#  AND WILL NOT DEFEND, INDEMNIFY, NOR HOLD YOU HARMLESS FOR ANY CLAIMS\n",
    "#  ARISING FROM OR RELATED TO THE CODE; AND (D)WITH RESPECT TO YOUR EXERCISE\n",
    "#  OF ANY RIGHTS GRANTED TO YOU FOR THE CODE, CLOUDERA IS NOT LIABLE FOR ANY\n",
    "#  DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, PUNITIVE OR\n",
    "#  CONSEQUENTIAL DAMAGES INCLUDING, BUT NOT LIMITED TO, DAMAGES\n",
    "#  RELATED TO LOST REVENUE, LOST PROFITS, LOSS OF INCOME, LOSS OF\n",
    "#  BUSINESS ADVANTAGE OR UNAVAILABILITY, OR LOSS OR CORRUPTION OF\n",
    "#  DATA.\n",
    "#\n",
    "# #  Author(s): Paul de Fusco\n",
    "#***************************************************************************/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62129517-d293-48da-b86d-2706d3389828",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import mlflow.spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc112bb6-2be3-412a-a2d2-89d17632be5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import sys\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52af5015-3f0f-43dd-b7fb-b5301d2c781f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import json\n",
    "import shutil\n",
    "import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "import cml.data_v1 as cmldata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d95bfea5-eec9-4ef7-a26b-b5928deebd4f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/05 19:47:22 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.\n",
      "23/11/05 19:47:22 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.\n",
      "Setting spark.hadoop.yarn.resourcemanager.principal to pauldefusco\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/05 19:47:23 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.\n",
      "23/11/05 19:47:23 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.\n",
      "23/11/05 19:47:23 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.\n",
      "23/11/05 19:47:24 WARN JettyUtils: GET /jobs/ failed: java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.produce(EatWhatYouKill.java:135)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/11/05 19:47:24 WARN HttpChannel: /jobs/\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.produce(EatWhatYouKill.java:135)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/11/05 19:47:25 WARN JettyUtils: GET /jobs/ failed: java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n",
      "\tat org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:383)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/11/05 19:47:25 WARN HttpChannel: /jobs/\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n",
      "\tat org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:383)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/11/05 19:47:25 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "23/11/05 19:47:26 WARN JettyUtils: GET /jobs/ failed: java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n",
      "\tat org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:383)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/11/05 19:47:26 WARN HttpChannel: /jobs/\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n",
      "\tat org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:383)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/11/05 19:47:27 WARN JettyUtils: GET /jobs/ failed: java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n",
      "\tat org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:383)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/11/05 19:47:27 WARN HttpChannel: /jobs/\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n",
      "\tat org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:383)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/11/05 19:47:28 WARN HiveServer2CredentialProvider: Failed to get HS2 delegation token\n",
      "java.util.NoSuchElementException: spark.sql.hive.hiveserver2.jdbc.url\n",
      "\tat org.apache.spark.SparkConf.$anonfun$get$1(SparkConf.scala:245)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.SparkConf.get(SparkConf.scala:245)\n",
      "\tat com.hortonworks.spark.deploy.yarn.security.HiveServer2CredentialProvider.obtainDelegationTokens(HiveServer2CredentialProvider.scala:64)\n",
      "\tat org.apache.spark.deploy.security.HadoopDelegationTokenManager.$anonfun$obtainDelegationTokens$2(HadoopDelegationTokenManager.scala:164)\n",
      "\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat scala.collection.MapLike$DefaultValuesIterable.foreach(MapLike.scala:213)\n",
      "\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)\n",
      "\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)\n",
      "\tat scala.collection.AbstractTraversable.flatMap(Traversable.scala:108)\n",
      "\tat org.apache.spark.deploy.security.HadoopDelegationTokenManager.org$apache$spark$deploy$security$HadoopDelegationTokenManager$$obtainDelegationTokens(HadoopDelegationTokenManager.scala:162)\n",
      "\tat org.apache.spark.deploy.security.HadoopDelegationTokenManager$$anon$4.run(HadoopDelegationTokenManager.scala:226)\n",
      "\tat org.apache.spark.deploy.security.HadoopDelegationTokenManager$$anon$4.run(HadoopDelegationTokenManager.scala:224)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)\n",
      "\tat org.apache.spark.deploy.security.HadoopDelegationTokenManager.obtainTokensAndScheduleRenewal(HadoopDelegationTokenManager.scala:224)\n",
      "\tat org.apache.spark.deploy.security.HadoopDelegationTokenManager.org$apache$spark$deploy$security$HadoopDelegationTokenManager$$updateTokensTask(HadoopDelegationTokenManager.scala:198)\n",
      "\tat org.apache.spark.deploy.security.HadoopDelegationTokenManager.start(HadoopDelegationTokenManager.scala:123)\n",
      "\tat org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.$anonfun$start$1(CoarseGrainedSchedulerBackend.scala:552)\n",
      "\tat org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.$anonfun$start$1$adapted(CoarseGrainedSchedulerBackend.scala:549)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.start(CoarseGrainedSchedulerBackend.scala:549)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.KubernetesClusterSchedulerBackend.start(KubernetesClusterSchedulerBackend.scala:95)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:220)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:581)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/11/05 19:47:28 WARN JettyUtils: GET /jobs/ failed: java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n",
      "\tat org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:383)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/11/05 19:47:28 WARN HttpChannel: /jobs/\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n",
      "\tat org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:383)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/11/05 19:47:29 WARN ExecutorAllocationManager: Dynamic allocation without a shuffle service is an experimental feature.\n",
      "23/11/05 19:47:29 WARN JettyUtils: GET /jobs/ failed: java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n",
      "\tat org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:383)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/11/05 19:47:29 WARN HttpChannel: /jobs/\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n",
      "\tat org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:383)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/11/05 19:47:33 WARN HiveClientImpl: Detected HiveConf hive.execution.engine is 'tez' and will be reset to 'mr' to disable useless hive logic\n",
      "Hive Session ID = 75a34268-c698-42fb-8b4d-c3dd714e05a2\n",
      "23/11/05 19:47:35 WARN HiveMetaStoreClient: Failed to connect to the MetaStore Server...\n",
      "23/11/05 19:47:35 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           namespace|\n",
      "+--------------------+\n",
      "|         01_car_data|\n",
      "|           01_car_dw|\n",
      "|              adb101|\n",
      "|            airlines|\n",
      "|        airlines_csv|\n",
      "|    airlines_iceberg|\n",
      "|airlines_iceberg_...|\n",
      "|      airlines_mjain|\n",
      "|          airquality|\n",
      "|                ajvp|\n",
      "|          atlas_demo|\n",
      "|            bankdemo|\n",
      "|          bca_jps_l0|\n",
      "|     bri_ranger_demo|\n",
      "|cde_demo_pauldefusco|\n",
      "|   cde_demo_pdefusco|\n",
      "|        cde_workshop|\n",
      "|cde_workshop_pdef...|\n",
      "| cde_workshop_smohan|\n",
      "|             cdedemo|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cml.data_v1 as cmldata\n",
    "\n",
    "# Sample in-code customization of spark configurations\n",
    "#from pyspark import SparkContext\n",
    "#SparkContext.setSystemProperty('spark.executor.cores', '1')\n",
    "#SparkContext.setSystemProperty('spark.executor.memory', '2g')\n",
    "\n",
    "CONNECTION_NAME = \"go01-aw-dl\"\n",
    "conn = cmldata.get_connection(CONNECTION_NAME)\n",
    "spark = conn.get_spark_session()\n",
    "\n",
    "# Sample usage to run query through spark\n",
    "EXAMPLE_SQL_QUERY = \"show databases\"\n",
    "spark.sql(EXAMPLE_SQL_QUERY).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4378e42e-8b7a-4687-a98b-c4d765a13289",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_df = spark.createDataFrame(\n",
    "[\n",
    "    (\"0\", \"a b c d e spark\", 1.0),\n",
    "    (\"1\", \"b d\", 0.0),\n",
    "    (\"2\", \"spark f g h\", 1.0),\n",
    "    (\"3\", \"hadoop mapreduce\", 0.0),\n",
    "],\n",
    "[\"id\", \"text\", \"label\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7affbc3-c9fd-4f9b-b817-e3e79596ab46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def exp1(df):\n",
    "\n",
    "    mlflow.set_experiment(\"sparkml-experiment\")\n",
    "\n",
    "    ##EXPERIMENT 1\n",
    "\n",
    "    df.writeTo(\"spark_catalog.default.training\").using(\"iceberg\").createOrReplace()\n",
    "    spark.sql(\"SELECT * FROM spark_catalog.default.training\").show()\n",
    "\n",
    "    ### SHOW TABLE HISTORY AND SNAPSHOTS\n",
    "    spark.read.format(\"iceberg\").load(\"spark_catalog.default.training.history\").show(20, False)\n",
    "    spark.read.format(\"iceberg\").load(\"spark_catalog.default.training.snapshots\").show(20, False)\n",
    "\n",
    "    snapshot_id = spark.read.format(\"iceberg\").load(\"spark_catalog.default.training.snapshots\").select(\"snapshot_id\").tail(1)[0][0]\n",
    "    committed_at = spark.read.format(\"iceberg\").load(\"spark_catalog.default.training.snapshots\").select(\"committed_at\").tail(1)[0][0].strftime('%m/%d/%Y')\n",
    "    parent_id = spark.read.format(\"iceberg\").load(\"spark_catalog.default.training.snapshots\").select(\"parent_id\").tail(1)[0][0]\n",
    "    \n",
    "    tags = {\n",
    "      \"iceberg_snapshot_id\": snapshot_id,\n",
    "      \"iceberg_snapshot_committed_at\": committed_at,\n",
    "      \"iceberg_parent_id\": parent_id,\n",
    "      \"row_count\": training_df.count()\n",
    "    }\n",
    "    \n",
    "    ### MLFLOW EXPERIMENT RUN\n",
    "    with mlflow.start_run() as run:\n",
    "\n",
    "        maxIter=8\n",
    "        regParam=0.01\n",
    "\n",
    "        tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "        hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "        lr = LogisticRegression(maxIter=maxIter, regParam=regParam)\n",
    "        pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "        model = pipeline.fit(training_df)\n",
    "\n",
    "        mlflow.log_param(\"maxIter\", maxIter)\n",
    "        mlflow.log_param(\"regParam\", regParam)\n",
    "\n",
    "        #prediction = model.transform(test)\n",
    "        mlflow.set_tags(tags)\n",
    "\n",
    "    mlflow.end_run()\n",
    "    \n",
    "    experiment_id = mlflow.get_experiment_by_name(\"sparkml-experiment\").experiment_id\n",
    "    runs_df = mlflow.search_runs(experiment_id, run_view_type=1)\n",
    "    \n",
    "    return runs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2be8d293-25d5-453d-a3af-c4a41f1a24ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def exp2(df):\n",
    "    \n",
    "    mlflow.set_experiment(\"sparkml-experiment\")\n",
    "    \n",
    "    ##EXPERIMENT 2\n",
    "\n",
    "    ### ICEBERG INSERT DATA - APPEND FROM DATAFRAME\n",
    "\n",
    "    # PRE-INSERT\n",
    "    spark.sql(\"SELECT * FROM spark_catalog.default.training\").show()\n",
    "\n",
    "    temp_df = spark.sql(\"SELECT * FROM spark_catalog.default.training\")\n",
    "    temp_df.writeTo(\"spark_catalog.default.training\").append()\n",
    "    df = spark.sql(\"SELECT * FROM spark_catalog.default.training\")\n",
    "\n",
    "    # PROST-INSERT\n",
    "    spark.sql(\"SELECT * FROM spark_catalog.default.training\").show()\n",
    "\n",
    "    spark.read.format(\"iceberg\").load(\"spark_catalog.default.training.history\").show(20, False)\n",
    "    spark.read.format(\"iceberg\").load(\"spark_catalog.default.training.snapshots\").show(20, False)\n",
    "\n",
    "    snapshot_id = spark.read.format(\"iceberg\").load(\"spark_catalog.default.training.snapshots\").select(\"snapshot_id\").tail(1)[0][0]\n",
    "    committed_at = spark.read.format(\"iceberg\").load(\"spark_catalog.default.training.snapshots\").select(\"committed_at\").tail(1)[0][0].strftime('%m/%d/%Y')\n",
    "    parent_id = spark.read.format(\"iceberg\").load(\"spark_catalog.default.training.snapshots\").select(\"parent_id\").tail(1)[0][0]\n",
    "    \n",
    "    tags = {\n",
    "      \"iceberg_snapshot_id\": snapshot_id,\n",
    "      \"iceberg_snapshot_committed_at\": committed_at,\n",
    "      \"iceberg_parent_id\": parent_id,\n",
    "      \"row_count\": df.count()\n",
    "    }\n",
    "    \n",
    "    ### MLFLOW EXPERIMENT RUN\n",
    "    with mlflow.start_run() as run:\n",
    "\n",
    "        maxIter=10\n",
    "        regParam=0.002\n",
    "\n",
    "        tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "        hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "        lr = LogisticRegression(maxIter=maxIter, regParam=regParam)\n",
    "        pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "        model = pipeline.fit(training_df)\n",
    "\n",
    "        mlflow.log_param(\"maxIter\", maxIter)\n",
    "        mlflow.log_param(\"regParam\", regParam)\n",
    "\n",
    "        #prediction = model.transform(test)\n",
    "        mlflow.set_tags(tags)\n",
    "\n",
    "    mlflow.end_run()\n",
    "    \n",
    "    experiment_id = mlflow.get_experiment_by_name(\"sparkml-experiment\").experiment_id\n",
    "    runs_df = mlflow.search_runs(experiment_id, run_view_type=1)\n",
    "    \n",
    "    return runs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "789b6807-d337-4451-9433-be4dfd71bb96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def exp3(df, snapshot_id):\n",
    "    ##EXPERIMENT 3\n",
    "\n",
    "    df = spark.read.option(\"snapshot-id\", snapshot_id).table(\"spark_catalog.default.training\")\n",
    "\n",
    "    committed_at = spark.sql(\"SELECT committed_at FROM spark_catalog.default.training.snapshots WHERE snapshot_id = {};\".format(snapshot_id)).collect()[0][0].strftime('%m/%d/%Y')\n",
    "    parent_id = str(spark.sql(\"SELECT parent_id FROM spark_catalog.default.training.snapshots WHERE snapshot_id = {};\".format(snapshot_id)).tail(1)[0][0])\n",
    "\n",
    "    tags = {\n",
    "      \"iceberg_snapshot_id\": snapshot_id,\n",
    "      \"iceberg_snapshot_committed_at\": committed_at,\n",
    "      \"iceberg_parent_id\": parent_id,\n",
    "      \"row_count\": training_df.count()\n",
    "    }\n",
    "\n",
    "    ### MLFLOW EXPERIMENT RUN\n",
    "    with mlflow.start_run() as run:\n",
    "\n",
    "        maxIter=7\n",
    "        regParam=0.005\n",
    "\n",
    "        tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "        hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "        lr = LogisticRegression(maxIter=maxIter, regParam=regParam)\n",
    "        pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "        model = pipeline.fit(training_df)\n",
    "\n",
    "        mlflow.log_param(\"maxIter\", maxIter)\n",
    "        mlflow.log_param(\"regParam\", regParam)\n",
    "\n",
    "        #prediction = model.transform(test)\n",
    "        mlflow.set_tags(tags)\n",
    "\n",
    "    mlflow.end_run()\n",
    "    \n",
    "    experiment_id = mlflow.get_experiment_by_name(\"sparkml-experiment\").experiment_id\n",
    "    runs_df = mlflow.search_runs(experiment_id, run_view_type=1)\n",
    "\n",
    "    #spark.stop()\n",
    "    \n",
    "    return runs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a40c1893-56ce-4ef3-8680-720f4ffc379b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/11/05 19:47:41 INFO mlflow.tracking.fluent: Experiment with name 'sparkml-experiment' does not exist. Creating a new experiment.\n",
      "23/11/05 19:47:41 WARN HiveMetaStoreClient: Failed to connect to the MetaStore Server...\n",
      "23/11/05 19:47:58 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+-----+\n",
      "| id|            text|label|\n",
      "+---+----------------+-----+\n",
      "|  0| a b c d e spark|  1.0|\n",
      "|  1|             b d|  0.0|\n",
      "|  2|     spark f g h|  1.0|\n",
      "|  3|hadoop mapreduce|  0.0|\n",
      "+---+----------------+-----+\n",
      "\n",
      "+-----------------------+------------------+---------+-------------------+\n",
      "|made_current_at        |snapshot_id       |parent_id|is_current_ancestor|\n",
      "+-----------------------+------------------+---------+-------------------+\n",
      "|2023-11-05 19:48:13.242|426957395241265414|null     |true               |\n",
      "+-----------------------+------------------+---------+-------------------+\n",
      "\n",
      "+-----------------------+-------------------+-------------------+---------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|committed_at           |snapshot_id        |parent_id          |operation|manifest_list                                                                                                                                                         |summary                                                                                                                                                                                                                                                                                                     |\n",
      "+-----------------------+-------------------+-------------------+---------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|2023-03-10 20:22:22.662|2693814795059767550|null               |append   |s3a://go01-demo/warehouse/tablespace/external/hive/cdh_test/user/hive/warehouse/training/metadata/snap-2693814795059767550-1-26bf3275-0195-45d6-a92d-c80f445cc568.avro|{spark.app.id -> local-1678478239199, added-data-files -> 2, added-records -> 4, added-files-size -> 1822, changed-partition-count -> 1, total-records -> 4, total-files-size -> 1822, total-data-files -> 2, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}            |\n",
      "|2023-03-10 20:23:04.731|2957939241357099335|2693814795059767550|append   |s3a://go01-demo/warehouse/tablespace/external/hive/cdh_test/user/hive/warehouse/training/metadata/snap-2957939241357099335-1-4665298e-07c6-4164-b65a-6538058b2e18.avro|{spark.app.id -> local-1678478239199, added-data-files -> 1, added-records -> 4, added-files-size -> 996, changed-partition-count -> 1, total-records -> 8, total-files-size -> 2818, total-data-files -> 3, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}             |\n",
      "|2023-03-31 14:51:03.217|3843157946227379170|null               |append   |s3a://go01-demo/warehouse/tablespace/external/hive/cdh_test/user/hive/warehouse/training/metadata/snap-3843157946227379170-1-fd02f7c5-abf3-4b89-8ee2-dc76d16746c9.avro|{spark.app.id -> spark-application-1680274218303, added-data-files -> 2, added-records -> 4, added-files-size -> 1822, changed-partition-count -> 1, total-records -> 4, total-files-size -> 1822, total-data-files -> 2, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}|\n",
      "|2023-03-31 14:51:13.942|5852654135945392377|3843157946227379170|append   |s3a://go01-demo/warehouse/tablespace/external/hive/cdh_test/user/hive/warehouse/training/metadata/snap-5852654135945392377-1-dc55f98f-017c-40a0-9d13-81831f2e2780.avro|{spark.app.id -> spark-application-1680274218303, added-data-files -> 1, added-records -> 4, added-files-size -> 996, changed-partition-count -> 1, total-records -> 8, total-files-size -> 2818, total-data-files -> 3, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0} |\n",
      "|2023-11-05 19:48:13.242|426957395241265414 |null               |append   |s3a://go01-demo/warehouse/tablespace/external/hive/cdh_test/user/hive/warehouse/training/metadata/snap-426957395241265414-1-34ed070c-e000-49ae-aa29-d11b16ed6986.avro |{spark.app.id -> spark-application-1699213645495, added-data-files -> 2, added-records -> 4, added-files-size -> 1822, changed-partition-count -> 1, total-records -> 4, total-files-size -> 1822, total-data-files -> 2, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}|\n",
      "+-----------------------+-------------------+-------------------+---------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/05 19:48:23 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "23/11/05 19:48:23 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n",
      "23/11/05 19:48:23 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "23/11/05 19:48:23 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>status</th>\n",
       "      <th>artifact_uri</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>params.regParam</th>\n",
       "      <th>params.maxIter</th>\n",
       "      <th>tags.iceberg_snapshot_id</th>\n",
       "      <th>tags.iceberg_snapshot_committed_at</th>\n",
       "      <th>tags.mlflow.source.type</th>\n",
       "      <th>tags.mlflow.source.git.commit</th>\n",
       "      <th>tags.mlflow.source.name</th>\n",
       "      <th>tags.mlflow.user</th>\n",
       "      <th>tags.iceberg_parent_id</th>\n",
       "      <th>tags.row_count</th>\n",
       "      <th>tags.engineID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>toi7-h745-fb0i-o9pg</td>\n",
       "      <td>pbq3-oss4-0wox-z4x4</td>\n",
       "      <td>EXPERIMENT_RUN_FINISHED</td>\n",
       "      <td>/home/cdsw/.experiments/pbq3-oss4-0wox-z4x4/to...</td>\n",
       "      <td>2023-11-05 19:48:18.394889216+00:00</td>\n",
       "      <td>2023-11-05 19:48:24.708999936+00:00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>8</td>\n",
       "      <td>426957395241265414</td>\n",
       "      <td>11/05/2023</td>\n",
       "      <td>LOCAL</td>\n",
       "      <td>06b7bed0031ad636f6b3ade4189ea15b164906f3</td>\n",
       "      <td>/usr/local/lib/python3.9/site-packages/ipykern...</td>\n",
       "      <td>pauldefusco</td>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "      <td>3fw9f74t2egfv60p</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                run_id        experiment_id                   status  \\\n",
       "0  toi7-h745-fb0i-o9pg  pbq3-oss4-0wox-z4x4  EXPERIMENT_RUN_FINISHED   \n",
       "\n",
       "                                        artifact_uri  \\\n",
       "0  /home/cdsw/.experiments/pbq3-oss4-0wox-z4x4/to...   \n",
       "\n",
       "                           start_time                            end_time  \\\n",
       "0 2023-11-05 19:48:18.394889216+00:00 2023-11-05 19:48:24.708999936+00:00   \n",
       "\n",
       "  params.regParam params.maxIter tags.iceberg_snapshot_id  \\\n",
       "0            0.01              8       426957395241265414   \n",
       "\n",
       "  tags.iceberg_snapshot_committed_at tags.mlflow.source.type  \\\n",
       "0                         11/05/2023                   LOCAL   \n",
       "\n",
       "              tags.mlflow.source.git.commit  \\\n",
       "0  06b7bed0031ad636f6b3ade4189ea15b164906f3   \n",
       "\n",
       "                             tags.mlflow.source.name tags.mlflow.user  \\\n",
       "0  /usr/local/lib/python3.9/site-packages/ipykern...      pauldefusco   \n",
       "\n",
       "  tags.iceberg_parent_id tags.row_count     tags.engineID  \n",
       "0                   None              4  3fw9f74t2egfv60p  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp1(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2202c604-b3c8-4be1-8501-91cf83fa6065",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+-----+\n",
      "| id|            text|label|\n",
      "+---+----------------+-----+\n",
      "|  0| a b c d e spark|  1.0|\n",
      "|  1|             b d|  0.0|\n",
      "|  2|     spark f g h|  1.0|\n",
      "|  3|hadoop mapreduce|  0.0|\n",
      "+---+----------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+-----+\n",
      "| id|            text|label|\n",
      "+---+----------------+-----+\n",
      "|  0| a b c d e spark|  1.0|\n",
      "|  1|             b d|  0.0|\n",
      "|  2|     spark f g h|  1.0|\n",
      "|  3|hadoop mapreduce|  0.0|\n",
      "|  0| a b c d e spark|  1.0|\n",
      "|  1|             b d|  0.0|\n",
      "|  2|     spark f g h|  1.0|\n",
      "|  3|hadoop mapreduce|  0.0|\n",
      "+---+----------------+-----+\n",
      "\n",
      "+-----------------------+-------------------+------------------+-------------------+\n",
      "|made_current_at        |snapshot_id        |parent_id         |is_current_ancestor|\n",
      "+-----------------------+-------------------+------------------+-------------------+\n",
      "|2023-11-05 19:48:13.242|426957395241265414 |null              |true               |\n",
      "|2023-11-05 19:51:28.961|4225021400360977669|426957395241265414|true               |\n",
      "+-----------------------+-------------------+------------------+-------------------+\n",
      "\n",
      "+-----------------------+-------------------+-------------------+---------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|committed_at           |snapshot_id        |parent_id          |operation|manifest_list                                                                                                                                                         |summary                                                                                                                                                                                                                                                                                                     |\n",
      "+-----------------------+-------------------+-------------------+---------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|2023-03-10 20:22:22.662|2693814795059767550|null               |append   |s3a://go01-demo/warehouse/tablespace/external/hive/cdh_test/user/hive/warehouse/training/metadata/snap-2693814795059767550-1-26bf3275-0195-45d6-a92d-c80f445cc568.avro|{spark.app.id -> local-1678478239199, added-data-files -> 2, added-records -> 4, added-files-size -> 1822, changed-partition-count -> 1, total-records -> 4, total-files-size -> 1822, total-data-files -> 2, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}            |\n",
      "|2023-03-10 20:23:04.731|2957939241357099335|2693814795059767550|append   |s3a://go01-demo/warehouse/tablespace/external/hive/cdh_test/user/hive/warehouse/training/metadata/snap-2957939241357099335-1-4665298e-07c6-4164-b65a-6538058b2e18.avro|{spark.app.id -> local-1678478239199, added-data-files -> 1, added-records -> 4, added-files-size -> 996, changed-partition-count -> 1, total-records -> 8, total-files-size -> 2818, total-data-files -> 3, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}             |\n",
      "|2023-03-31 14:51:03.217|3843157946227379170|null               |append   |s3a://go01-demo/warehouse/tablespace/external/hive/cdh_test/user/hive/warehouse/training/metadata/snap-3843157946227379170-1-fd02f7c5-abf3-4b89-8ee2-dc76d16746c9.avro|{spark.app.id -> spark-application-1680274218303, added-data-files -> 2, added-records -> 4, added-files-size -> 1822, changed-partition-count -> 1, total-records -> 4, total-files-size -> 1822, total-data-files -> 2, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}|\n",
      "|2023-03-31 14:51:13.942|5852654135945392377|3843157946227379170|append   |s3a://go01-demo/warehouse/tablespace/external/hive/cdh_test/user/hive/warehouse/training/metadata/snap-5852654135945392377-1-dc55f98f-017c-40a0-9d13-81831f2e2780.avro|{spark.app.id -> spark-application-1680274218303, added-data-files -> 1, added-records -> 4, added-files-size -> 996, changed-partition-count -> 1, total-records -> 8, total-files-size -> 2818, total-data-files -> 3, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0} |\n",
      "|2023-11-05 19:48:13.242|426957395241265414 |null               |append   |s3a://go01-demo/warehouse/tablespace/external/hive/cdh_test/user/hive/warehouse/training/metadata/snap-426957395241265414-1-34ed070c-e000-49ae-aa29-d11b16ed6986.avro |{spark.app.id -> spark-application-1699213645495, added-data-files -> 2, added-records -> 4, added-files-size -> 1822, changed-partition-count -> 1, total-records -> 4, total-files-size -> 1822, total-data-files -> 2, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}|\n",
      "|2023-11-05 19:51:28.961|4225021400360977669|426957395241265414 |append   |s3a://go01-demo/warehouse/tablespace/external/hive/cdh_test/user/hive/warehouse/training/metadata/snap-4225021400360977669-1-7df2b18c-960f-42ea-b8bd-e5587bf1bb05.avro|{spark.app.id -> spark-application-1699213645495, added-data-files -> 1, added-records -> 4, added-files-size -> 996, changed-partition-count -> 1, total-records -> 8, total-files-size -> 2818, total-data-files -> 3, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0} |\n",
      "+-----------------------+-------------------+-------------------+---------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>status</th>\n",
       "      <th>artifact_uri</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>params.regParam</th>\n",
       "      <th>params.maxIter</th>\n",
       "      <th>tags.iceberg_snapshot_id</th>\n",
       "      <th>tags.iceberg_snapshot_committed_at</th>\n",
       "      <th>tags.mlflow.source.type</th>\n",
       "      <th>tags.mlflow.source.git.commit</th>\n",
       "      <th>tags.mlflow.source.name</th>\n",
       "      <th>tags.mlflow.user</th>\n",
       "      <th>tags.iceberg_parent_id</th>\n",
       "      <th>tags.row_count</th>\n",
       "      <th>tags.engineID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>toi7-h745-fb0i-o9pg</td>\n",
       "      <td>pbq3-oss4-0wox-z4x4</td>\n",
       "      <td>EXPERIMENT_RUN_FINISHED</td>\n",
       "      <td>/home/cdsw/.experiments/pbq3-oss4-0wox-z4x4/to...</td>\n",
       "      <td>2023-11-05 19:48:18.394889216+00:00</td>\n",
       "      <td>2023-11-05 19:48:24.708999936+00:00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>8</td>\n",
       "      <td>426957395241265414</td>\n",
       "      <td>11/05/2023</td>\n",
       "      <td>LOCAL</td>\n",
       "      <td>06b7bed0031ad636f6b3ade4189ea15b164906f3</td>\n",
       "      <td>/usr/local/lib/python3.9/site-packages/ipykern...</td>\n",
       "      <td>pauldefusco</td>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "      <td>3fw9f74t2egfv60p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bokv-w00w-q6zu-zvsy</td>\n",
       "      <td>pbq3-oss4-0wox-z4x4</td>\n",
       "      <td>EXPERIMENT_RUN_FINISHED</td>\n",
       "      <td>/home/cdsw/.experiments/pbq3-oss4-0wox-z4x4/bo...</td>\n",
       "      <td>2023-11-05 19:51:31.007222016+00:00</td>\n",
       "      <td>2023-11-05 19:51:36.945999872+00:00</td>\n",
       "      <td>0.002</td>\n",
       "      <td>10</td>\n",
       "      <td>4225021400360977669</td>\n",
       "      <td>11/05/2023</td>\n",
       "      <td>LOCAL</td>\n",
       "      <td>06b7bed0031ad636f6b3ade4189ea15b164906f3</td>\n",
       "      <td>/usr/local/lib/python3.9/site-packages/ipykern...</td>\n",
       "      <td>pauldefusco</td>\n",
       "      <td>426957395241265414</td>\n",
       "      <td>8</td>\n",
       "      <td>3fw9f74t2egfv60p</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                run_id        experiment_id                   status  \\\n",
       "0  toi7-h745-fb0i-o9pg  pbq3-oss4-0wox-z4x4  EXPERIMENT_RUN_FINISHED   \n",
       "1  bokv-w00w-q6zu-zvsy  pbq3-oss4-0wox-z4x4  EXPERIMENT_RUN_FINISHED   \n",
       "\n",
       "                                        artifact_uri  \\\n",
       "0  /home/cdsw/.experiments/pbq3-oss4-0wox-z4x4/to...   \n",
       "1  /home/cdsw/.experiments/pbq3-oss4-0wox-z4x4/bo...   \n",
       "\n",
       "                           start_time                            end_time  \\\n",
       "0 2023-11-05 19:48:18.394889216+00:00 2023-11-05 19:48:24.708999936+00:00   \n",
       "1 2023-11-05 19:51:31.007222016+00:00 2023-11-05 19:51:36.945999872+00:00   \n",
       "\n",
       "  params.regParam params.maxIter tags.iceberg_snapshot_id  \\\n",
       "0            0.01              8       426957395241265414   \n",
       "1           0.002             10      4225021400360977669   \n",
       "\n",
       "  tags.iceberg_snapshot_committed_at tags.mlflow.source.type  \\\n",
       "0                         11/05/2023                   LOCAL   \n",
       "1                         11/05/2023                   LOCAL   \n",
       "\n",
       "              tags.mlflow.source.git.commit  \\\n",
       "0  06b7bed0031ad636f6b3ade4189ea15b164906f3   \n",
       "1  06b7bed0031ad636f6b3ade4189ea15b164906f3   \n",
       "\n",
       "                             tags.mlflow.source.name tags.mlflow.user  \\\n",
       "0  /usr/local/lib/python3.9/site-packages/ipykern...      pauldefusco   \n",
       "1  /usr/local/lib/python3.9/site-packages/ipykern...      pauldefusco   \n",
       "\n",
       "  tags.iceberg_parent_id tags.row_count     tags.engineID  \n",
       "0                   None              4  3fw9f74t2egfv60p  \n",
       "1     426957395241265414              8  3fw9f74t2egfv60p  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp2(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e3d4a03-cd39-4940-bf7e-9dd151828bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>status</th>\n",
       "      <th>artifact_uri</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>params.maxIter</th>\n",
       "      <th>params.regParam</th>\n",
       "      <th>tags.mlflow.source.git.commit</th>\n",
       "      <th>tags.mlflow.source.name</th>\n",
       "      <th>tags.mlflow.user</th>\n",
       "      <th>tags.mlflow.source.type</th>\n",
       "      <th>tags.row_count</th>\n",
       "      <th>tags.iceberg_parent_id</th>\n",
       "      <th>tags.iceberg_snapshot_id</th>\n",
       "      <th>tags.iceberg_snapshot_committed_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kfnc-iesm-r79x-nqcp</td>\n",
       "      <td>p1y7-kv7u-fkya-ca0e</td>\n",
       "      <td>EXPERIMENT_RUN_RUNNING</td>\n",
       "      <td>/home/cdsw/.experiments/p1y7-kv7u-fkya-ca0e/kf...</td>\n",
       "      <td>2023-03-29 01:04:44.571115008+00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1d14407660f321086ef27b91b0e06ca91cd34b18</td>\n",
       "      <td>/usr/local/lib/python3.7/site-packages/ipykern...</td>\n",
       "      <td>pauldefusco</td>\n",
       "      <td>LOCAL</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vaza-u8gd-dbkd-k4ow</td>\n",
       "      <td>p1y7-kv7u-fkya-ca0e</td>\n",
       "      <td>EXPERIMENT_RUN_FAILED</td>\n",
       "      <td>/home/cdsw/.experiments/p1y7-kv7u-fkya-ca0e/va...</td>\n",
       "      <td>2023-03-29 01:06:53.296070144+00:00</td>\n",
       "      <td>2023-03-29 01:06:56.807000064+00:00</td>\n",
       "      <td>8</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1d14407660f321086ef27b91b0e06ca91cd34b18</td>\n",
       "      <td>/usr/local/lib/python3.7/site-packages/ipykern...</td>\n",
       "      <td>pauldefusco</td>\n",
       "      <td>LOCAL</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>55oy-jgoj-sxwp-tp7b</td>\n",
       "      <td>p1y7-kv7u-fkya-ca0e</td>\n",
       "      <td>EXPERIMENT_RUN_FINISHED</td>\n",
       "      <td>/home/cdsw/.experiments/p1y7-kv7u-fkya-ca0e/55...</td>\n",
       "      <td>2023-03-29 01:09:43.688317184+00:00</td>\n",
       "      <td>2023-03-29 01:09:46.864000+00:00</td>\n",
       "      <td>8</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1d14407660f321086ef27b91b0e06ca91cd34b18</td>\n",
       "      <td>/usr/local/lib/python3.7/site-packages/ipykern...</td>\n",
       "      <td>pauldefusco</td>\n",
       "      <td>LOCAL</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "      <td>5150219139107687331</td>\n",
       "      <td>03/29/2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bmxd-ywal-739v-3ikq</td>\n",
       "      <td>p1y7-kv7u-fkya-ca0e</td>\n",
       "      <td>EXPERIMENT_RUN_FINISHED</td>\n",
       "      <td>/home/cdsw/.experiments/p1y7-kv7u-fkya-ca0e/bm...</td>\n",
       "      <td>2023-03-29 01:11:43.908041984+00:00</td>\n",
       "      <td>2023-03-29 01:11:48.220000+00:00</td>\n",
       "      <td>10</td>\n",
       "      <td>0.002</td>\n",
       "      <td>1d14407660f321086ef27b91b0e06ca91cd34b18</td>\n",
       "      <td>/usr/local/lib/python3.7/site-packages/ipykern...</td>\n",
       "      <td>pauldefusco</td>\n",
       "      <td>LOCAL</td>\n",
       "      <td>8</td>\n",
       "      <td>5150219139107687331</td>\n",
       "      <td>8171776268264279750</td>\n",
       "      <td>03/29/2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tndc-jcty-7qn9-jxsu</td>\n",
       "      <td>p1y7-kv7u-fkya-ca0e</td>\n",
       "      <td>EXPERIMENT_RUN_FINISHED</td>\n",
       "      <td>/home/cdsw/.experiments/p1y7-kv7u-fkya-ca0e/tn...</td>\n",
       "      <td>2023-03-29 01:14:32.279436032+00:00</td>\n",
       "      <td>2023-03-29 01:14:35.438000128+00:00</td>\n",
       "      <td>7</td>\n",
       "      <td>0.005</td>\n",
       "      <td>1d14407660f321086ef27b91b0e06ca91cd34b18</td>\n",
       "      <td>/usr/local/lib/python3.7/site-packages/ipykern...</td>\n",
       "      <td>pauldefusco</td>\n",
       "      <td>LOCAL</td>\n",
       "      <td>4</td>\n",
       "      <td>5150219139107687331</td>\n",
       "      <td>8171776268264279750</td>\n",
       "      <td>03/29/2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4yk0-cwcg-9hcn-c7k9</td>\n",
       "      <td>p1y7-kv7u-fkya-ca0e</td>\n",
       "      <td>EXPERIMENT_RUN_FINISHED</td>\n",
       "      <td>/home/cdsw/.experiments/p1y7-kv7u-fkya-ca0e/4y...</td>\n",
       "      <td>2023-03-29 01:15:29.313084928+00:00</td>\n",
       "      <td>2023-03-29 01:15:32.180000+00:00</td>\n",
       "      <td>7</td>\n",
       "      <td>0.005</td>\n",
       "      <td>1d14407660f321086ef27b91b0e06ca91cd34b18</td>\n",
       "      <td>/usr/local/lib/python3.7/site-packages/ipykern...</td>\n",
       "      <td>pauldefusco</td>\n",
       "      <td>LOCAL</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "      <td>5150219139107687331</td>\n",
       "      <td>03/29/2023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                run_id        experiment_id                   status  \\\n",
       "0  kfnc-iesm-r79x-nqcp  p1y7-kv7u-fkya-ca0e   EXPERIMENT_RUN_RUNNING   \n",
       "1  vaza-u8gd-dbkd-k4ow  p1y7-kv7u-fkya-ca0e    EXPERIMENT_RUN_FAILED   \n",
       "2  55oy-jgoj-sxwp-tp7b  p1y7-kv7u-fkya-ca0e  EXPERIMENT_RUN_FINISHED   \n",
       "3  bmxd-ywal-739v-3ikq  p1y7-kv7u-fkya-ca0e  EXPERIMENT_RUN_FINISHED   \n",
       "4  tndc-jcty-7qn9-jxsu  p1y7-kv7u-fkya-ca0e  EXPERIMENT_RUN_FINISHED   \n",
       "5  4yk0-cwcg-9hcn-c7k9  p1y7-kv7u-fkya-ca0e  EXPERIMENT_RUN_FINISHED   \n",
       "\n",
       "                                        artifact_uri  \\\n",
       "0  /home/cdsw/.experiments/p1y7-kv7u-fkya-ca0e/kf...   \n",
       "1  /home/cdsw/.experiments/p1y7-kv7u-fkya-ca0e/va...   \n",
       "2  /home/cdsw/.experiments/p1y7-kv7u-fkya-ca0e/55...   \n",
       "3  /home/cdsw/.experiments/p1y7-kv7u-fkya-ca0e/bm...   \n",
       "4  /home/cdsw/.experiments/p1y7-kv7u-fkya-ca0e/tn...   \n",
       "5  /home/cdsw/.experiments/p1y7-kv7u-fkya-ca0e/4y...   \n",
       "\n",
       "                           start_time                            end_time  \\\n",
       "0 2023-03-29 01:04:44.571115008+00:00                                 NaT   \n",
       "1 2023-03-29 01:06:53.296070144+00:00 2023-03-29 01:06:56.807000064+00:00   \n",
       "2 2023-03-29 01:09:43.688317184+00:00    2023-03-29 01:09:46.864000+00:00   \n",
       "3 2023-03-29 01:11:43.908041984+00:00    2023-03-29 01:11:48.220000+00:00   \n",
       "4 2023-03-29 01:14:32.279436032+00:00 2023-03-29 01:14:35.438000128+00:00   \n",
       "5 2023-03-29 01:15:29.313084928+00:00    2023-03-29 01:15:32.180000+00:00   \n",
       "\n",
       "  params.maxIter params.regParam             tags.mlflow.source.git.commit  \\\n",
       "0             10           0.001  1d14407660f321086ef27b91b0e06ca91cd34b18   \n",
       "1              8            0.01  1d14407660f321086ef27b91b0e06ca91cd34b18   \n",
       "2              8            0.01  1d14407660f321086ef27b91b0e06ca91cd34b18   \n",
       "3             10           0.002  1d14407660f321086ef27b91b0e06ca91cd34b18   \n",
       "4              7           0.005  1d14407660f321086ef27b91b0e06ca91cd34b18   \n",
       "5              7           0.005  1d14407660f321086ef27b91b0e06ca91cd34b18   \n",
       "\n",
       "                             tags.mlflow.source.name tags.mlflow.user  \\\n",
       "0  /usr/local/lib/python3.7/site-packages/ipykern...      pauldefusco   \n",
       "1  /usr/local/lib/python3.7/site-packages/ipykern...      pauldefusco   \n",
       "2  /usr/local/lib/python3.7/site-packages/ipykern...      pauldefusco   \n",
       "3  /usr/local/lib/python3.7/site-packages/ipykern...      pauldefusco   \n",
       "4  /usr/local/lib/python3.7/site-packages/ipykern...      pauldefusco   \n",
       "5  /usr/local/lib/python3.7/site-packages/ipykern...      pauldefusco   \n",
       "\n",
       "  tags.mlflow.source.type tags.row_count tags.iceberg_parent_id  \\\n",
       "0                   LOCAL           None                   None   \n",
       "1                   LOCAL           None                   None   \n",
       "2                   LOCAL              4                   None   \n",
       "3                   LOCAL              8    5150219139107687331   \n",
       "4                   LOCAL              4    5150219139107687331   \n",
       "5                   LOCAL              4                   None   \n",
       "\n",
       "  tags.iceberg_snapshot_id tags.iceberg_snapshot_committed_at  \n",
       "0                     None                               None  \n",
       "1                     None                               None  \n",
       "2      5150219139107687331                         03/29/2023  \n",
       "3      8171776268264279750                         03/29/2023  \n",
       "4      8171776268264279750                         03/29/2023  \n",
       "5      5150219139107687331                         03/29/2023  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Retrieve snapshot_id from Experiments page or above dataframe. Use the Snapshot ID from the first experiment.\n",
    "snapshot_id = \"5150219139107687331\"\n",
    "exp3(training_df, snapshot_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8421fe81-03a0-4b04-8a37-08af974e3d75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
