{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2de9681-c386-43e7-b1ad-91476ca199a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "import sys\n",
    "from pyspark.sql import Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "575acd08-8aeb-4d21-adb9-cfd94a02de9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting spark.hadoop.yarn.resourcemanager.principal to pauldefusco\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Car Sales Report\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\")\\\n",
    "    .config(\"spark.sql.catalog.spark_catalog.type\", \"hive\")\\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\\\n",
    "    .config(\"spark.yarn.access.hadoopFileSystems\", \"s3a://go01-demo/\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10505093-cf23-4014-9887-6823c5c412d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.conf.get(\"spark.jars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38f8dc4b-6119-4491-8c1c-c5e607125489",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lake_name = \"s3a://go01-demo/\"\n",
    "s3BucketName = \"s3a://go01-demo/cde-workshop/cardata-csv/\"\n",
    "# Your Username Here:\n",
    "username = \"test_user_112222_7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8137dc4-6e1b-4507-b215-1d825a15490c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'default'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.currentDatabase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "447d9209-7c73-4e03-b3d3-79c7c906618c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"inferSchema\", \"true\").option(\"header\", \"true\").csv(\"/home/cdsw/data/batch_data_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c93e9249-2790-464c-9101-55db3f3acce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+-------------------+--------+--------------------+--------------------+-------+-------+-------------+---------+---------+-----------+------------+---------------+------------------+\n",
      "|              name|      street_address|               city|postcode|        phone_number|                 job|recency|history|used_discount|used_bogo| zip_code|is_referral|     channel|          offer|             score|\n",
      "+------------------+--------------------+-------------------+--------+--------------------+--------------------+-------+-------+-------------+---------+---------+-----------+------------+---------------+------------------+\n",
      "|      Austin Duran|     5204 Emily Club|     Lake Mollyside|   79396| +1-070-516-3006x662|   Film/video editor|      5| 171.13|            0|        1|    Urban|          0|       Phone|Buy One Get One|1.0335466196578111|\n",
      "|      Austin Smith| 97370 William Vista|        Lake Nicole|   98418|     +1-901-430-6901| Corporate treasurer|      5| 425.29|            1|        1|    Urban|          0|         Web|       Discount|0.7220789384296671|\n",
      "|     Antonio Brown|81973 Mills Branc...|South Angelaborough|   87498|    641.609.5045x102|     Legal secretary|     10| 489.05|            0|        1|    Urban|          0|         Web|Buy One Get One|1.6104761991672558|\n",
      "|  Cassandra Wright|    898 Lamb Freeway|          Marymouth|   59839|        173-981-5102|IT sales professi...|     10|  29.99|            0|        1|Surburban|          1|         Web|       Discount|1.2705092191627845|\n",
      "|       Holly Gates|    0286 Adams Point|  South Andrewmouth|   38078|          5533889641|             Barista|      5| 336.36|            0|        1|Surburban|          0|         Web|       Discount|0.9793397117305144|\n",
      "|    Ashley Fischer|641 Damon Station...|         Lake Sarah|   44654|       (740)187-3099| Designer, jewellery|      9| 197.55|            0|        1|Surburban|          0|         Web|       No Offer| 0.613567130118007|\n",
      "|    Meagan Gardner|    86405 Sarah Fork|         Whitemouth|   16820|+1-873-649-6288x4408|Engineer, control...|      3|  70.37|            0|        1|Surburban|          1|       Phone|Buy One Get One|1.9170438954324958|\n",
      "|      Curtis Davis|82024 Khan Shore ...|      Sullivanburgh|   98837|    746-101-7087x456|Health and safety...|     10|  51.62|            0|        1|Surburban|          0|       Phone|       No Offer|0.6738909762883862|\n",
      "|      Lonnie Berry|82232 Banks Radia...|      Velasquezstad|   35878|        446.234.5553|   Recycling officer|      2| 396.14|            0|        1|Surburban|          0|       Phone|       No Offer|0.8729147847029389|\n",
      "|       Katie Olsen|77808 Nicholas Sh...|     Port Sarahaven|   64290|        999-851-8341|Trade union resea...|      9|  29.99|            1|        0|    Urban|          0|       Phone|       No Offer|1.4941849081561258|\n",
      "|     Erik Gonzalez|68970 Davis Fords...|           New Adam|   35362|    040-735-2000x174| Associate Professor|      6| 451.07|            1|        1|Surburban|          0|       Phone|       No Offer|0.8815557427593754|\n",
      "|     Melissa White|17670 Adrienne Co...|         Combsville|   23630|   271-886-8185x5780| Colour technologist|      3|  29.99|            0|        1|Surburban|          0|       Phone|       Discount|0.9412874118535336|\n",
      "| Michelle Robinson|   2852 Thomas Knoll|         Valdezview|   52958|    310-169-8445x113|IT technical supp...|      1| 103.32|            1|        0|Surburban|          0|         Web|       Discount| 1.414499807446299|\n",
      "|          Ana Rios|98550 Kevin Exten...|          Morseview|     769|    001-643-685-3730|Accountant, chart...|      1|  38.85|            1|        0|Surburban|          0|       Phone|       Discount|0.6206540500021109|\n",
      "|     Emily Richard|5788 Eric Groves ...|          Maysmouth|   84179|     +1-317-028-0652|Air traffic contr...|      1| 261.19|            0|        1|    Urban|          1|       Phone|       Discount|0.8968041347978893|\n",
      "|       Shawn Clark|30513 Austin Circ...|       North Shelby|   15781|          8939818176|       Hotel manager|      1| 207.64|            1|        0|    Urban|          0|Multichannel|Buy One Get One|1.0165760697396629|\n",
      "|Valerie Washington|47397 Michael Cre...|  North Johnchester|   19392|       (802)062-0452|             Midwife|      7|  29.99|            1|        0|    Urban|          0|         Web|       No Offer|1.4200311199936566|\n",
      "|  Kristi Young DDS|32165 Michele Cre...|         Meganhaven|   95642|     +1-326-530-3308|Clinical research...|     10| 315.73|            0|        1|Surburban|          0|Multichannel|       No Offer|  1.07749067687023|\n",
      "|       David Kelly|     42284 Henry Row|          New Kevin|   80270|001-604-607-8265x...| Corporate treasurer|      1| 899.62|            1|        1|    Urban|          1|       Phone|Buy One Get One|0.8508750670817035|\n",
      "|        Luis Munoz|518 Ronald Shores...|          New Scott|   74960|+1-006-146-0537x3373|Volunteer coordin...|      1|  60.51|            1|        0|Surburban|          1|         Web|       No Offer|0.3631822018426999|\n",
      "+------------------+--------------------+-------------------+--------+--------------------+--------------------+-------+-------+-------------+---------+---------+-----------+------------+---------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0aa5ec7-59b6-48bb-8042-70967a6a901a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('name', 'string'),\n",
       " ('street_address', 'string'),\n",
       " ('city', 'string'),\n",
       " ('postcode', 'int'),\n",
       " ('phone_number', 'string'),\n",
       " ('job', 'string'),\n",
       " ('recency', 'int'),\n",
       " ('history', 'double'),\n",
       " ('used_discount', 'int'),\n",
       " ('used_bogo', 'int'),\n",
       " ('zip_code', 'string'),\n",
       " ('is_referral', 'int'),\n",
       " ('channel', 'string'),\n",
       " ('offer', 'string'),\n",
       " ('score', 'double')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c5f0d28-c064-4abd-9bce-01b201056440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS default.batch_data_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9197edf-066c-493c-8b20-4265e3228dad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE TABLE IF NOT EXISTS spark_catalog.default.batch_data_test (name string, street_address string, city string, postcode int, phone_number string, job string, recency int, history double, used_discount int, used_bogo int, zip_code string, is_referral int, channel string, offer string, score double) USING iceberg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f34b632-f3ae-4ba0-bdbb-7a28f2991e42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c5ede94c-b1fe-4e5f-84d5-9ce4864f34a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/06 01:03:14 498 ERROR TaskSetManager: Task 0 in stage 9.0 failed 4 times; aborting job\n",
      "23/01/06 01:03:14 500 ERROR AtomicReplaceTableAsSelectExec: Data source write support IcebergBatchWrite(table=default.icetest, format=PARQUET) is aborting.\n",
      "23/01/06 01:03:14 500 ERROR AtomicReplaceTableAsSelectExec: Data source write support IcebergBatchWrite(table=default.icetest, format=PARQUET) aborted.\n",
      "23/01/06 01:03:14 500 ERROR Utils: Aborting task\n",
      "org.apache.spark.SparkException: Writing job aborted\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.writingJobAbortedError(QueryExecutionErrors.scala:613)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:389)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:333)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.writeWithV2(WriteToDataSourceV2Exec.scala:193)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.$anonfun$writeToTable$1(WriteToDataSourceV2Exec.scala:483)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.writeToTable(WriteToDataSourceV2Exec.scala:471)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.writeToTable$(WriteToDataSourceV2Exec.scala:466)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.writeToTable(WriteToDataSourceV2Exec.scala:193)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.run(WriteToDataSourceV2Exec.scala:224)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:111)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:111)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:129)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.runCommand(DataFrameWriterV2.scala:194)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.internalReplace(DataFrameWriterV2.scala:211)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.createOrReplace(DataFrameWriterV2.scala:132)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 4 times, most recent failure: Lost task 0.3 in stage 9.0 (TID 23) (100.100.82.186 executor 5): java.lang.ClassNotFoundException: org.apache.iceberg.spark.source.SparkWrite$WriterFactory\n",
      "\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n",
      "\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n",
      "\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n",
      "\tat java.lang.Class.forName0(Native Method)\n",
      "\tat java.lang.Class.forName(Class.java:348)\n",
      "\tat org.apache.spark.serializer.JavaDeserializationStream$$anon$1.resolveClass(JavaSerializer.scala:68)\n",
      "\tat java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1988)\n",
      "\tat java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1852)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2186)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n",
      "\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2119)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1657)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n",
      "\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:503)\n",
      "\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:461)\n",
      "\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)\n",
      "\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:115)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:83)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:357)\n",
      "\t... 47 more\n",
      "Caused by: java.lang.ClassNotFoundException: org.apache.iceberg.spark.source.SparkWrite$WriterFactory\n",
      "\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n",
      "\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n",
      "\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n",
      "\tat java.lang.Class.forName0(Native Method)\n",
      "\tat java.lang.Class.forName(Class.java:348)\n",
      "\tat org.apache.spark.serializer.JavaDeserializationStream$$anon$1.resolveClass(JavaSerializer.scala:68)\n",
      "\tat java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1988)\n",
      "\tat java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1852)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2186)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n",
      "\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2119)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1657)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n",
      "\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:503)\n",
      "\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:461)\n",
      "\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)\n",
      "\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:115)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:83)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o169.createOrReplace.\n: org.apache.spark.SparkException: Writing job aborted\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.writingJobAbortedError(QueryExecutionErrors.scala:613)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:389)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:333)\n\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.writeWithV2(WriteToDataSourceV2Exec.scala:193)\n\tat org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.$anonfun$writeToTable$1(WriteToDataSourceV2Exec.scala:483)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n\tat org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.writeToTable(WriteToDataSourceV2Exec.scala:471)\n\tat org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.writeToTable$(WriteToDataSourceV2Exec.scala:466)\n\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.writeToTable(WriteToDataSourceV2Exec.scala:193)\n\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.run(WriteToDataSourceV2Exec.scala:224)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:111)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:129)\n\tat org.apache.spark.sql.DataFrameWriterV2.runCommand(DataFrameWriterV2.scala:194)\n\tat org.apache.spark.sql.DataFrameWriterV2.internalReplace(DataFrameWriterV2.scala:211)\n\tat org.apache.spark.sql.DataFrameWriterV2.createOrReplace(DataFrameWriterV2.scala:132)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 4 times, most recent failure: Lost task 0.3 in stage 9.0 (TID 23) (100.100.82.186 executor 5): java.lang.ClassNotFoundException: org.apache.iceberg.spark.source.SparkWrite$WriterFactory\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\tat java.lang.Class.forName0(Native Method)\n\tat java.lang.Class.forName(Class.java:348)\n\tat org.apache.spark.serializer.JavaDeserializationStream$$anon$1.resolveClass(JavaSerializer.scala:68)\n\tat java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1988)\n\tat java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1852)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2186)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2119)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1657)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:503)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:461)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)\n\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:115)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:83)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:357)\n\t... 47 more\nCaused by: java.lang.ClassNotFoundException: org.apache.iceberg.spark.source.SparkWrite$WriterFactory\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\tat java.lang.Class.forName0(Native Method)\n\tat java.lang.Class.forName(Class.java:348)\n\tat org.apache.spark.serializer.JavaDeserializationStream$$anon$1.resolveClass(JavaSerializer.scala:68)\n\tat java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1988)\n\tat java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1852)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2186)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2119)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1657)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:503)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:461)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)\n\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:115)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:83)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_199/1293156851.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriteTo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark_catalog.default.icetest\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"iceberg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateOrReplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#df.writeTo(\"spark_catalog.default.batch_data_test\").append()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcreateOrReplace\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1149\u001b[0m         \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtable\u001b[0m \u001b[0mexists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mits\u001b[0m \u001b[0mconfiguration\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdata\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mreplaced\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \"\"\"\n\u001b[0;32m-> 1151\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateOrReplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o169.createOrReplace.\n: org.apache.spark.SparkException: Writing job aborted\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.writingJobAbortedError(QueryExecutionErrors.scala:613)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:389)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:333)\n\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.writeWithV2(WriteToDataSourceV2Exec.scala:193)\n\tat org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.$anonfun$writeToTable$1(WriteToDataSourceV2Exec.scala:483)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n\tat org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.writeToTable(WriteToDataSourceV2Exec.scala:471)\n\tat org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.writeToTable$(WriteToDataSourceV2Exec.scala:466)\n\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.writeToTable(WriteToDataSourceV2Exec.scala:193)\n\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.run(WriteToDataSourceV2Exec.scala:224)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:111)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:129)\n\tat org.apache.spark.sql.DataFrameWriterV2.runCommand(DataFrameWriterV2.scala:194)\n\tat org.apache.spark.sql.DataFrameWriterV2.internalReplace(DataFrameWriterV2.scala:211)\n\tat org.apache.spark.sql.DataFrameWriterV2.createOrReplace(DataFrameWriterV2.scala:132)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 4 times, most recent failure: Lost task 0.3 in stage 9.0 (TID 23) (100.100.82.186 executor 5): java.lang.ClassNotFoundException: org.apache.iceberg.spark.source.SparkWrite$WriterFactory\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\tat java.lang.Class.forName0(Native Method)\n\tat java.lang.Class.forName(Class.java:348)\n\tat org.apache.spark.serializer.JavaDeserializationStream$$anon$1.resolveClass(JavaSerializer.scala:68)\n\tat java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1988)\n\tat java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1852)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2186)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2119)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1657)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:503)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:461)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)\n\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:115)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:83)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:357)\n\t... 47 more\nCaused by: java.lang.ClassNotFoundException: org.apache.iceberg.spark.source.SparkWrite$WriterFactory\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\tat java.lang.Class.forName0(Native Method)\n\tat java.lang.Class.forName(Class.java:348)\n\tat org.apache.spark.serializer.JavaDeserializationStream$$anon$1.resolveClass(JavaSerializer.scala:68)\n\tat java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1988)\n\tat java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1852)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2186)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2119)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1657)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:503)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:461)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)\n\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:115)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:83)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "df.writeTo(\"spark_catalog.default.icetest\").using(\"iceberg\").createOrReplace()\n",
    "#df.writeTo(\"spark_catalog.default.batch_data_test\").append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75240ed3-9f6b-4b7e-9e31-55497790adfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "55c8fc85-548b-49cc-97ae-1e999e4829f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.mode(\"overwrite\").saveAsTable('default.iceberg_example_010523'.format(username), format=\"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dbb548a5-0a10-48a5-8b35-f37ec9b06650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"ALTER TABLE default.iceberg_example_010523 UNSET TBLPROPERTIES ('TRANSLATED_TO_EXTERNAL')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b1d4b3de-6229-408a-97d1-a25e2b94619b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[migrated_files_count: bigint]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CALL spark_catalog.system.migrate('default.iceberg_example_010523')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "60930806-ad88-40b2-a960-354f7f3b21e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/06 00:52:34 683 ERROR TaskSetManager: Task 0 in stage 8.0 failed 4 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o166.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 8.0 failed 4 times, most recent failure: Lost task 0.3 in stage 8.0 (TID 19) (100.100.82.184 executor 4): java.lang.ClassNotFoundException: org.apache.iceberg.spark.source.SparkScan$ReadTask\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\tat java.lang.Class.forName0(Native Method)\n\tat java.lang.Class.forName(Class.java:348)\n\tat org.apache.spark.serializer.JavaDeserializationStream$$anon$1.resolveClass(JavaSerializer.scala:68)\n\tat java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1988)\n\tat java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1852)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2186)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:503)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:461)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)\n\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:115)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3715)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2728)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2728)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2935)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:287)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:326)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.ClassNotFoundException: org.apache.iceberg.spark.source.SparkScan$ReadTask\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\tat java.lang.Class.forName0(Native Method)\n\tat java.lang.Class.forName(Class.java:348)\n\tat org.apache.spark.serializer.JavaDeserializationStream$$anon$1.resolveClass(JavaSerializer.scala:68)\n\tat java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1988)\n\tat java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1852)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2186)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:503)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:461)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)\n\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:115)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_199/1724531193.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SELECT * FROM spark_catalog.default.iceberg_example_010523.PARTITIONS\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o166.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 8.0 failed 4 times, most recent failure: Lost task 0.3 in stage 8.0 (TID 19) (100.100.82.184 executor 4): java.lang.ClassNotFoundException: org.apache.iceberg.spark.source.SparkScan$ReadTask\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\tat java.lang.Class.forName0(Native Method)\n\tat java.lang.Class.forName(Class.java:348)\n\tat org.apache.spark.serializer.JavaDeserializationStream$$anon$1.resolveClass(JavaSerializer.scala:68)\n\tat java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1988)\n\tat java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1852)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2186)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:503)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:461)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)\n\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:115)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3715)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2728)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2728)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2935)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:287)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:326)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.ClassNotFoundException: org.apache.iceberg.spark.source.SparkScan$ReadTask\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\tat java.lang.Class.forName0(Native Method)\n\tat java.lang.Class.forName(Class.java:348)\n\tat org.apache.spark.serializer.JavaDeserializationStream$$anon$1.resolveClass(JavaSerializer.scala:68)\n\tat java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1988)\n\tat java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1852)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2186)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:503)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:461)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)\n\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:115)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM spark_catalog.default.iceberg_example_010523.PARTITIONS\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd74916a-75fb-4460-8ae3-f80435e020e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443c943d-1977-48c3-8a9f-e565aa1df88e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6bb22096-b1c3-488b-958c-66dc504db724",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Table(name='t', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='s', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='lc_smote_subset', database='default', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='driver_stats', database='default', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='table1', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='ttable2', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='employee_ofer', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='tax_auth_heb', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='results_anomaly', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='mytable', database='default', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='transactions', database='default', description=None, tableType=None, isTemporary=False),\n",
       " Table(name='customer_temp', database='default', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='ms_churn_prototype', database='default', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='customers', database='default', description=None, tableType=None, isTemporary=False),\n",
       " Table(name='table2', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='numbers', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='a', database='default', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='tax_auth_heb_line', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='syslog', database='default', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='wc_matches', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='transactions_data_2', database='default', description=None, tableType=None, isTemporary=False),\n",
       " Table(name='customers_2', database='default', description=None, tableType=None, isTemporary=False),\n",
       " Table(name='fraudulent_txn_2', database='default', description=None, tableType=None, isTemporary=False),\n",
       " Table(name='telco_churn', database='default', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='a_demo_table', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='anomaly_detection', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='lc_smote_k_3', database='default', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='b', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='churn_prototype_atlas', database='default', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='hurricane', database='default', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='hurricane3', database='default', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='churn_prototype', database='default', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='churn_prototype_username', database='default', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='total_airtime_ext', database='default', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='international_results_skip', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='fraudulent_txn', database='default', description=None, tableType=None, isTemporary=False),\n",
       " Table(name='cc_info_table', database='default', description='Credit Card Info', tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='transactions_data', database='default', description=None, tableType=None, isTemporary=False),\n",
       " Table(name='a1', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='b1', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='customerdata', database='default', description=None, tableType=None, isTemporary=False),\n",
       " Table(name='hurricane2', database='default', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='people', database='default', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='lc_predictions', database='default', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='profile_1670606388', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='erik_test', database='default', description=None, tableType=None, isTemporary=False),\n",
       " Table(name='01_customer_data', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='fraud_trans', database='default', description=None, tableType=None, isTemporary=False),\n",
       " Table(name='customerdata_temp', database='default', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='airlines_csv1flights_csv', database='default', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='lin1', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='pk_inline', database='default', description=None, tableType=None, isTemporary=False),\n",
       " Table(name='lin2', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='batch_load_table', database='default', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='transactions2', database='default', description=None, tableType=None, isTemporary=False),\n",
       " Table(name='emp_fname', database='default', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='emp_all', database='default', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='emp_younger', database='default', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='emp_older', database='default', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='member', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='validationtable', database='default', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='covid_states', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='lin3', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='pysparktab', database='default', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='flights', database='default', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='emp_lname', database='default', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='emp_age', database='default', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='emp_denom', database='default', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='emp_id', database='default', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='tb_funcionario', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='tb_orc', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='mlops_staging_table', database='default', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='wsp_flights', database='default', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='table_pessoa', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='table_idade', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='secret', database='default', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='marketing_campaign_table', database='default', description='Marketing Campaign', tableType='MANAGED', isTemporary=False),\n",
       " Table(name='million', database='default', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='adiboncdp', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='million2', database='default', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='churn_prototype_rtheron', database='default', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='historical_customers', database='default', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='lc_train', database='default', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='customerdatag', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='customerdata1g', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='lc_test', database='default', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='e2edemo', database='default', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='clickthrough', database='default', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='clickthrough_with_ts', database='default', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='clickthrough_csv_batch_20141026', database='default', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='test', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='lc_smote_all', database='default', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='paul_lc_table', database='default', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='demo', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='bank_info_table', database='default', description='Bank Info', tableType='MANAGED', isTemporary=False),\n",
       " Table(name='adibusage', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='paul_lc_model_scoring', database='default', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='wc_forecasts', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='customer_daily_profile', database='default', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='si2', database='default', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='lc_table', database='default', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='tbl_test_jr', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='batchdata', database='default', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='customerrecords', database='default', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='ti', database='default', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='si', database='default', description=None, tableType='EXTERNAL', isTemporary=False)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b4f3c764-0e92-4045-8060-0a6501b2ac0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+-----------+-----------+---------------+--------------------+\n",
      "|    event_timestamp|driver_id|  conv_rate|   acc_rate|avg_daily_trips|             created|\n",
      "+-------------------+---------+-----------+-----------+---------------+--------------------+\n",
      "|2022-12-06 08:00:00|     1003| 0.74933994|  0.9661571|            246|2022-12-21 08:33:...|\n",
      "|2022-12-06 09:00:00|     1003|  0.4900406| 0.79025567|            629|2022-12-21 08:33:...|\n",
      "|2022-12-06 10:00:00|     1003| 0.44497514|  0.7003286|            409|2022-12-21 08:33:...|\n",
      "|2022-12-06 11:00:00|     1003| 0.83675057|0.023251861|            301|2022-12-21 08:33:...|\n",
      "|2022-12-06 12:00:00|     1003| 0.60384226|  0.8531334|            159|2022-12-21 08:33:...|\n",
      "|2022-12-06 13:00:00|     1003|  0.3236954| 0.18340571|              1|2022-12-21 08:33:...|\n",
      "|2022-12-06 14:00:00|     1003|  0.0521478| 0.30485526|            293|2022-12-21 08:33:...|\n",
      "|2022-12-06 15:00:00|     1003|  0.9964056| 0.36764374|            908|2022-12-21 08:33:...|\n",
      "|2022-12-06 16:00:00|     1003|0.077008195|  0.6614635|            806|2022-12-21 08:33:...|\n",
      "|2022-12-06 17:00:00|     1003| 0.45955628|0.071994975|            896|2022-12-21 08:33:...|\n",
      "|2022-12-06 18:00:00|     1003| 0.32990307| 0.39433748|            488|2022-12-21 08:33:...|\n",
      "|2022-12-06 19:00:00|     1003| 0.42810535| 0.53178465|            922|2022-12-21 08:33:...|\n",
      "|2022-12-06 20:00:00|     1003| 0.08885972| 0.83320296|            423|2022-12-21 08:33:...|\n",
      "|2022-12-06 21:00:00|     1003|  0.7748199| 0.42605558|            351|2022-12-21 08:33:...|\n",
      "|2022-12-06 22:00:00|     1003| 0.17716327| 0.60617745|            535|2022-12-21 08:33:...|\n",
      "|2022-12-06 23:00:00|     1003| 0.67718524| 0.07141108|            923|2022-12-21 08:33:...|\n",
      "|2022-12-07 00:00:00|     1003|  0.2357042|  0.6364419|            205|2022-12-21 08:33:...|\n",
      "|2022-12-07 01:00:00|     1003| 0.52954143|0.009542442|             49|2022-12-21 08:33:...|\n",
      "|2022-12-07 02:00:00|     1003|  0.4665318| 0.42768332|            126|2022-12-21 08:33:...|\n",
      "|2022-12-07 03:00:00|     1003| 0.28530097| 0.76767653|            740|2022-12-21 08:33:...|\n",
      "+-------------------+---------+-----------+-----------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#---------------------------------------------------\n",
    "#               LOAD ICEBERG TABLES AS DATAFRAMES\n",
    "#---------------------------------------------------\n",
    "spark.sql(\"SELECT * FROM spark_catalog.default.batchdataiceberg\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "84655aef-d440-4734-a41a-8704f173eab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+\n",
      "|      catalog|namespace|\n",
      "+-------------+---------+\n",
      "|spark_catalog|  default|\n",
      "+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW CURRENT NAMESPACE\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f1ff1cfe-d376-4ee0-b9d6-c4f37f47f3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql(\"USE spark_catalog.{}_CAR_DATA\".format(username))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d7581277-4a37-45cd-8622-8f3ca69946fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql(\"SHOW CURRENT NAMESPACE\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bc64c5b8-1112-4158-a450-a056964398d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql(\"SELECT * FROM spark_catalog.{}_CAR_DATA.CAR_SALES.history\".format(username)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5920c793-217a-43f2-9022-fea20985bb6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHOW ICEBERG TABLE HISTORY\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "spark_catalog requires a single-part namespace, but got [default, batchdata]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_199/997453683.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SHOW ICEBERG TABLE HISTORY\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#print(\"SELECT * FROM spark_catalog.default.table_pessoa.history;\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SELECT * FROM default.batchdata.history\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m         \"\"\"\n\u001b[0;32m--> 723\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    724\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtableName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: spark_catalog requires a single-part namespace, but got [default, batchdata]"
     ]
    }
   ],
   "source": [
    "# ICEBERG TABLE HISTORY (SHOWS EACH SNAPSHOT AND TIMESTAMP)\n",
    "print(\"SHOW ICEBERG TABLE HISTORY\")\n",
    "#print(\"SELECT * FROM spark_catalog.default.table_pessoa.history;\")\n",
    "spark.sql(\"SELECT * FROM spark_catalog.default.batchdataiceberg.history\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f06b7a51-2854-44fb-94de-596ac7c906c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_job_body(*runtime_addon_ids):\n",
    "    print(type(runtime_addon_ids))\n",
    "    print(runtime_addon_ids)\n",
    "    ls = list(runtime_addon_ids)\n",
    "    print(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5556c7ba-e5a6-4d09-b6d0-fb665bbdd9c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "('yoyo', 'yeh', 'haha')\n",
      "['yoyo', 'yeh', 'haha']\n"
     ]
    }
   ],
   "source": [
    "create_job_body(\"yoyo\", \"yeh\", \"haha\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8afb3f-93a4-42ff-acba-ccaa661725f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
